{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate datasets\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jan-hq/instruction-speech-conversation\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataset with shortest sound_convo for faster inference\n",
    "def sample_shortest_sample(num_samples):\n",
    "    # sort based on len(sound_convo)\n",
    "    sorted_samples = sorted(dataset, key=lambda x: len(x['sound_convo']))\n",
    "    return sorted_samples[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "checkpoint = \"jan-hq/llama-3-sound-instruct-cp\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Config model\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    load_in_8bit=True, # load model in 8bit\n",
    ")\n",
    "\n",
    "# Base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token = \"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of samples to test\n",
    "NO_TESTING_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import json\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": NO_TESTING_SAMPLES,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "sorted_samples = sample_shortest_sample(100)\n",
    "result = []\n",
    "\n",
    "for sample in sorted_samples:\n",
    "  messages = [\n",
    "      {\"role\": \"user\", \"content\": sample['sound_convo'][0]['content']}\n",
    "  ]\n",
    "  output = pipe(messages, **generation_args)\n",
    "  answer = output[0]['generated_text']\n",
    "  result.append({\n",
    "      \"prompt\": sample['prompt'],\n",
    "      \"answer\": sample['answer'],\n",
    "      \"output\": answer\n",
    "  })\n",
    "  with open(\"./results.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=4)\n",
    "  print(\"Model output:\", output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
