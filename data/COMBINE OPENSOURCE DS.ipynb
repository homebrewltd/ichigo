{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1a27f3-69ac-4bdd-8eff-f21d00b75f06",
   "metadata": {},
   "source": [
    "# Notebook to combine opensource dataset into one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd6b6f",
   "metadata": {},
   "source": [
    "List of datasets:\n",
    "- Intel/orca_dpo_pairs\n",
    "- routellm/gpt4_dataset\n",
    "- nomic-ai/gpt4all-j-prompt-generations\n",
    "- microsoft/orca-math-word-problems-200k\n",
    "- allenai/WildChat-1M\n",
    "- Open-Orca/oo-gpt4-200k\n",
    "- Magpie-Align/Magpie-Pro-300K-Filtered\n",
    "- qiaojin/PubMedQA\n",
    "- Undi95/Capybara-ShareGPT\n",
    "- HannahRoseKirk/prism-alignment - utterances\n",
    "- Magpie-Align/Magpie-Pro-300K-Filtered\n",
    "- BAAI/Infinity-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd04800",
   "metadata": {},
   "source": [
    "# I. Prepare OS datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0e9ea",
   "metadata": {},
   "source": [
    "## 1. Intel/orca_dpo_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_1 = load_dataset(\"Intel/orca_dpo_pairs\",split='train', token=\"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\")\n",
    "dataset_1 = dataset_1.remove_columns([col for col in dataset_1.column_names if col not in ['question','chosen']])\n",
    "dataset_1 = dataset_1.rename_column(\"question\", \"prompt\")\n",
    "dataset_1 = dataset_1.rename_column(\"chosen\", \"answer\")\n",
    "print(dataset_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f14785",
   "metadata": {},
   "source": [
    "## 2. routellm/gpt4_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3f6dd-31a3-46ac-9e16-48bcf4543c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_2 = load_dataset(\"routellm/gpt4_dataset\",split='train', token=\"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\")\n",
    "dataset_2 = dataset_2.remove_columns([col for col in dataset_2.column_names if col not in ['prompt','gpt4_response']])\n",
    "dataset_2 = dataset_2.rename_column(\"gpt4_response\", \"answer\")\n",
    "print(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f01c3",
   "metadata": {},
   "source": [
    "## 3. nomic-ai/gpt4all-j-prompt-generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02574c2-62d5-4ab9-947f-b86c78189fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_3 = load_dataset(\"nomic-ai/gpt4all-j-prompt-generations\",split='train', token=\"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\")\n",
    "dataset_3 = dataset_3.remove_columns([col for col in dataset_3.column_names if col not in ['prompt','response']])\n",
    "dataset_3 = dataset_3.rename_column(\"response\", \"answer\")\n",
    "print(dataset_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bdfca",
   "metadata": {},
   "source": [
    "## 4. microsoft/orca-math-word-problems-200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca33ee1-8e22-4612-8d06-cdbc4f0a97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_4 = load_dataset(\"microsoft/orca-math-word-problems-200k\",split='train')\n",
    "dataset_4 = dataset_4.remove_columns([col for col in dataset_4.column_names if col not in ['question', 'answer']])\n",
    "dataset_4 = dataset_4.rename_column(\"question\", \"prompt\")\n",
    "print(dataset_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac28487",
   "metadata": {},
   "source": [
    "## 5. allenai/WildChat-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7f6d5-4cd5-442f-a7c4-537cd72fbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_5 = load_dataset(\"allenai/WildChat-1M\",split='train', token=\"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\")\n",
    "# Filter the dataset based on multiple conditions\n",
    "filtered_dataset = dataset_5.filter(lambda example: \n",
    "    example['model'] in [\"gpt-4-1106-preview\", \"gpt-4-0314\"] and\n",
    "    example['language'] == \"English\" and\n",
    "    example['turn'] == 1\n",
    ")\n",
    "\n",
    "# Print the filtered dataset to check the changes\n",
    "filtered_dataset = filtered_dataset.remove_columns([col for col in filtered_dataset.column_names if col not in ['conversation']])\n",
    "from datasets import load_dataset\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversation']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['role'] == 'user':\n",
    "                prompt = entry['content']\n",
    "            elif entry['role'] == 'assistant':\n",
    "                answer = entry['content']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_5 = filtered_dataset.map(extract_conversation, batched=True, num_proc=16).remove_columns(\"conversation\")\n",
    "# Display the first example to verify\n",
    "print(dataset_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f1326",
   "metadata": {},
   "source": [
    "## 6. Open-Orca/oo-gpt4-200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c0a42-00b1-4521-b64c-657cb0bf02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_6 = load_dataset(\"Open-Orca/oo-gpt4-200k\",split='train', token=\"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\")\n",
    "dataset_6 = dataset_6.remove_columns([col for col in dataset_6.column_names if col not in ['question','response']])\n",
    "dataset_6 = dataset_6.rename_column(\"question\", \"prompt\")\n",
    "dataset_6 = dataset_6.rename_column(\"response\", \"answer\")\n",
    "print(dataset_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0303ad",
   "metadata": {},
   "source": [
    "## 7. Magpie-Align/Magpie-Pro-300K-Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f541a-22e9-4d81-8377-fca39f31e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_7 = load_dataset(\"Magpie-Align/Magpie-Pro-300K-Filtered\",split='train', token=\"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\")\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_7 = dataset_7.map(extract_conversation, batched=True, num_proc=16).remove_columns([\"conversations\", \"uuid\"])\n",
    "\n",
    "# Display the first example to verify\n",
    "print(dataset_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde274d",
   "metadata": {},
   "source": [
    "## 8. qiaojin/PubMedQA - pqa_artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e4fa7-7605-42bc-8f5f-6f4480dd1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_8 = load_dataset(\"qiaojin/PubMedQA\",'pqa_artificial',split='train', token=\"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\")\n",
    "dataset_8 = dataset_8.remove_columns([col for col in dataset_8.column_names if col not in ['question','long_answer']])\n",
    "dataset_8 = dataset_8.rename_column(\"question\", \"prompt\")\n",
    "dataset_8 = dataset_8.rename_column(\"long_answer\", \"answer\")\n",
    "print(dataset_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ec670",
   "metadata": {},
   "source": [
    "## 9. Undi95/Capybara-ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a867a-b69c-4cfc-851b-bbd87439ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_9 = load_dataset(\"Undi95/Capybara-ShareGPT\",split='train', token=\"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\")\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_9 = dataset_9.map(extract_conversation, batched=True, num_proc=16).remove_columns([\"conversations\"])\n",
    "\n",
    "# Display the first example to verify\n",
    "print(dataset_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100c3d",
   "metadata": {},
   "source": [
    "## 10. HannahRoseKirk/prism-alignment - utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca5524-c028-431e-b768-cd167ae3fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_10 = load_dataset(\"HannahRoseKirk/prism-alignment\",'utterances',split='train', token=\"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\")\n",
    "dataset_10 = dataset_10.filter(lambda x: x['if_chosen'])\n",
    "dataset_10 = dataset_10.remove_columns([col for col in dataset_10.column_names if col not in ['user_prompt','model_response']])\n",
    "dataset_10 = dataset_10.rename_column(\"user_prompt\", \"prompt\")\n",
    "dataset_10 = dataset_10.rename_column(\"model_response\", \"answer\")\n",
    "print(dataset_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb04f2e",
   "metadata": {},
   "source": [
    "## 11. BAAI/Infinity-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853105a-ceed-4cac-9a3a-76d661c3e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_11 = load_dataset('BAAI/Infinity-Instruct',split='train', token=\"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\")\n",
    "\n",
    "# Define the filter function\n",
    "def filter_sources(batch):\n",
    "    # Check if source is not one of the specified strings\n",
    "    return [source not in {\"OpenHermes-2.5\", \"Orca-math-word-problems-200k\"} for source in batch['source']]\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_dataset_11 = dataset_11.filter(filter_sources, batched=True, num_proc=32)\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_11 = filtered_dataset_11.map(extract_conversation, batched=True, num_proc=32).remove_columns([\"conversations\",\"source\", \"id\",\"target_value\"])\n",
    "dataset_11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbe159",
   "metadata": {},
   "source": [
    "# II. Analysis and Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa86068-8741-4d7f-b419-2d2c2f515b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset details\n",
    "# Assuming dataset_1 to dataset_10 are loaded and accessible\n",
    "for i in range(1, 12):\n",
    "    dataset = eval(f'dataset_{i}')  # dynamically access each dataset\n",
    "    print(f'Dataset_{i} details:')\n",
    "    print(f'Features: {dataset.column_names}')\n",
    "    print(f'Number of rows: {dataset.num_rows}')\n",
    "    print('----------------------------------------')  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24516582-546b-421a-8a59-75cc86671361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all the datasets\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Assume dataset_1 to dataset_10 are already loaded and have the same schema\n",
    "all_datasets = [eval(f'dataset_{i}') for i in range(1, 12)]\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "\n",
    "# Printing some details about the combined dataset to confirm\n",
    "print(f'Combined dataset features: {combined_dataset.column_names}')\n",
    "print(f'Number of rows in combined dataset: {combined_dataset.num_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b814a-2234-4b48-b244-a7ba1f61c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(batch):\n",
    "    # List of substrings to remove\n",
    "    substrings_to_remove = ['<|prompter|>', '<|endoftext|>', '<|assistant|>','<|endofprompt|>']\n",
    "    \n",
    "    # Process each prompt in the batch\n",
    "    cleaned_prompts = []\n",
    "    for prompt in batch['prompt']:\n",
    "        for substring in substrings_to_remove:\n",
    "            prompt = prompt.replace(substring, '')\n",
    "        cleaned_prompts.append(prompt)\n",
    "\n",
    "    # Update the batch with cleaned prompts\n",
    "    batch['prompt'] = cleaned_prompts\n",
    "    return batch\n",
    "\n",
    "# Apply the cleaning function to the dataset using batching and multiple processes\n",
    "cleaned_dataset = combined_dataset.map(clean_prompt, batched=True, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156e8c1-c43f-4452-9658-7a7b897bb28f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignore samples with more than 64 tokens\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "# Assume `encoding` is an object that has an `encode` method (like a tokenizer)\n",
    "current_index = 0  # This will track the index of the current sample\n",
    "\n",
    "def add_token_counts(example):\n",
    "    global current_index\n",
    "    try:\n",
    "        num_tokens = len(encoding.encode(example['prompt']))\n",
    "        example['length'] = num_tokens\n",
    "    except ValueError as e:\n",
    "        print(f\"Error at index {current_index}: {str(e)}\")\n",
    "        # Optionally, you can add error handling, such as skipping this entry\n",
    "        example['length'] = 0  # Setting default or error value\n",
    "    finally:\n",
    "        current_index += 1  # Increment index whether there was an error or not\n",
    "    return example\n",
    "\n",
    "# Applying the function to the dataset\n",
    "filtered_dataset = cleaned_dataset.map(add_token_counts, batched=False)\n",
    "\n",
    "# Example of the dataset entry with token counts\n",
    "print(filtered_dataset['prompt'][0], filtered_dataset['length'][0])\n",
    "print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01873f04-e470-4c7b-8494-d8bf607c1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to keep only samples with fewer than 64 tokens\n",
    "filtered_dataset = filtered_dataset.filter(lambda example: example['length'] < 64, batched=False)\n",
    "\n",
    "# Print the number of samples in the filtered dataset\n",
    "print(f\"Number of samples with contrain tokens: {len(filtered_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9de2f7-02a1-4a35-afd9-4e190507928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "# Download the language identification model\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c301378-5abb-4c33-b29b-e8afebefaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if the text is in English, handling None values\n",
    "def is_english(example):\n",
    "    text = example['prompt']\n",
    "    if text is None:\n",
    "        return False  # Skip None values\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with space\n",
    "    prediction, probability = model.predict(text)\n",
    "    return prediction[0] == '__label__eng_Latn' and probability[0] > 0.8  # Adjust threshold as needed\n",
    "\n",
    "# Apply the filter to the dataset\n",
    "english_dataset = filtered_dataset.filter(lambda example: is_english(example))\n",
    "\n",
    "# Example of the English-only dataset\n",
    "print(english_dataset[0])\n",
    "print(english_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadebd0-d05f-46d2-94df-e53dac5ab340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# filter samples not really good as a sound samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a41aac-a832-4b53-a5b4-3a4df34180a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.push_to_hub(\"jan-hq/instruction-speech-v1.5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
