{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1a27f3-69ac-4bdd-8eff-f21d00b75f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3694fad9688448d892b24e8f62065902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/196 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505730a3d493471a93978e32b3c24d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/36.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c9555877b240c085ec3bdcce40d9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 12859\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_1 = load_dataset(\"Intel/orca_dpo_pairs\",split='train', token=\"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\")\n",
    "dataset_1 = dataset_1.remove_columns([col for col in dataset_1.column_names if col not in ['question','chosen']])\n",
    "dataset_1 = dataset_1.rename_column(\"question\", \"prompt\")\n",
    "dataset_1 = dataset_1.rename_column(\"chosen\", \"answer\")\n",
    "print(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee3f6dd-31a3-46ac-9e16-48bcf4543c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97e6e1e1434ffc834b9ea8f58f5a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bac964fff94ee9b926e5eaa4975025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316bc231848345c2a39b778473573150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c264d325be4362a027ead34ecabe0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/109101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e2acd7d96f4d398bc7938613b419d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 109101\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_2 = load_dataset(\"routellm/gpt4_dataset\",split='train', token=\"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\")\n",
    "dataset_2 = dataset_2.remove_columns([col for col in dataset_2.column_names if col not in ['prompt','gpt4_response']])\n",
    "dataset_2 = dataset_2.rename_column(\"gpt4_response\", \"answer\")\n",
    "print(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02574c2-62d5-4ab9-947f-b86c78189fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dfb19973a04d50915cef42e73daa13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b707e9b908154f7e8e6c2b28baddfca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785cf78976494719b97a16f5c315dd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8947ff2cdda43fb95b7e8805d410c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b6b501d41c4aee9c554d4e26abcdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/77.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef0a47eb88d4efa994c8f5bf65bfcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/808812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 808812\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_3 = load_dataset(\"nomic-ai/gpt4all-j-prompt-generations\",split='train', token=\"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\")\n",
    "dataset_3 = dataset_3.remove_columns([col for col in dataset_3.column_names if col not in ['prompt','response']])\n",
    "dataset_3 = dataset_3.rename_column(\"response\", \"answer\")\n",
    "print(dataset_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca33ee1-8e22-4612-8d06-cdbc4f0a97d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed67630e57d47138487945b80a851d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461ecd415ef642cfa12cb4907adaa766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697329e5039a4f3287e6a3f44ea362d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 200035\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_4 = load_dataset(\"microsoft/orca-math-word-problems-200k\",split='train')\n",
    "dataset_4 = dataset_4.remove_columns([col for col in dataset_4.column_names if col not in ['question', 'answer']])\n",
    "dataset_4 = dataset_4.rename_column(\"question\", \"prompt\")\n",
    "print(dataset_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da7f6d5-4cd5-442f-a7c4-537cd72fbc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed806bf0f9444f5ae900ed602979b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47ce4b691a243d9801eab789f567580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a9896e442841e7adba19ce2bbc80ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29245c629aec4ba0ad5878ec23306bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/19 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b351787902ca4da8ae0bbb72fe15f04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1039785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefc2e97322342afa6e6ff841ccb0940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a1f81901e543dba0afec06b0084bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1039785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07b588a25b54d58b71bc46335c292e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/75992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 75992\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_5 = load_dataset(\"allenai/WildChat-1M\",split='train', token=\"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\")\n",
    "# Filter the dataset based on multiple conditions\n",
    "filtered_dataset = dataset_5.filter(lambda example: \n",
    "    example['model'] in [\"gpt-4-1106-preview\", \"gpt-4-0314\"] and\n",
    "    example['language'] == \"English\" and\n",
    "    example['turn'] == 1\n",
    ")\n",
    "\n",
    "# Print the filtered dataset to check the changes\n",
    "filtered_dataset = filtered_dataset.remove_columns([col for col in filtered_dataset.column_names if col not in ['conversation']])\n",
    "from datasets import load_dataset\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversation']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['role'] == 'user':\n",
    "                prompt = entry['content']\n",
    "            elif entry['role'] == 'assistant':\n",
    "                answer = entry['content']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_5 = filtered_dataset.map(extract_conversation, batched=True, num_proc=16).remove_columns(\"conversation\")\n",
    "# Display the first example to verify\n",
    "print(dataset_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876c0a42-00b1-4521-b64c-657cb0bf02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96677bd2cb9040d7b8999693f03a2bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/206M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550bc8b3b38947d086a987b016ce6c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 200023\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_6 = load_dataset(\"Open-Orca/oo-gpt4-200k\",split='train', token=\"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\")\n",
    "dataset_6 = dataset_6.remove_columns([col for col in dataset_6.column_names if col not in ['question','response']])\n",
    "dataset_6 = dataset_6.rename_column(\"question\", \"prompt\")\n",
    "dataset_6 = dataset_6.rename_column(\"response\", \"answer\")\n",
    "print(dataset_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670f541a-22e9-4d81-8377-fca39f31e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d6d7fa6d2a4f709d2652e4ac0d7ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29685b8abd114f2b8ffb837b498e144c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4bb7903fae42edb70c80465c20a665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de0baababe841f7a8d3987b68164130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3b898572aa4e6f9ff5dd4256a51ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/300000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2181912030fd4e43af170ef84151557c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/300000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 300000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_7 = load_dataset(\"Magpie-Align/Magpie-Pro-300K-Filtered\",split='train', token=\"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\")\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_7 = dataset_7.map(extract_conversation, batched=True, num_proc=16).remove_columns([\"conversations\", \"uuid\"])\n",
    "\n",
    "# Display the first example to verify\n",
    "print(dataset_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523e4fa7-7605-42bc-8f5f-6f4480dd1a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56827a894d46448fb007d5c3007f8a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3e49da5d5544d99ec9b225e23a0779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b6a209f9a541549f3eaae34cf98106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/211269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 211269\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_8 = load_dataset(\"qiaojin/PubMedQA\",'pqa_artificial',split='train', token=\"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\")\n",
    "dataset_8 = dataset_8.remove_columns([col for col in dataset_8.column_names if col not in ['question','long_answer']])\n",
    "dataset_8 = dataset_8.rename_column(\"question\", \"prompt\")\n",
    "dataset_8 = dataset_8.rename_column(\"long_answer\", \"answer\")\n",
    "print(dataset_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375a867a-b69c-4cfc-851b-bbd87439ad08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1137682578fa4933bae229dbfe1c6004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5eface907346cd8b842bfed44e478a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/80.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8832633b914c49af84c555db5cb2d7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f185723f6ff8459eb6e9364acd881422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/16006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 16006\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_9 = load_dataset(\"Undi95/Capybara-ShareGPT\",split='train', token=\"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\")\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_9 = dataset_9.map(extract_conversation, batched=True, num_proc=16).remove_columns([\"conversations\"])\n",
    "\n",
    "# Display the first example to verify\n",
    "print(dataset_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ca5524-c028-431e-b768-cd167ae3fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568b848b20b946ed9fa12ecc1545b221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a040ffbf6d7479db10672029edf9032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521524bc0b74404acaa38ae15e3d432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc25da63dfc4a3ab6a72849bee6a231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/68371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'answer'],\n",
      "    num_rows: 27437\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_10 = load_dataset(\"HannahRoseKirk/prism-alignment\",'utterances',split='train', token=\"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\")\n",
    "dataset_10 = dataset_10.filter(lambda x: x['if_chosen'])\n",
    "dataset_10 = dataset_10.remove_columns([col for col in dataset_10.column_names if col not in ['user_prompt','model_response']])\n",
    "dataset_10 = dataset_10.rename_column(\"user_prompt\", \"prompt\")\n",
    "dataset_10 = dataset_10.rename_column(\"model_response\", \"answer\")\n",
    "print(dataset_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0853105a-ceed-4cac-9a3a-76d661c3e040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198b44fa800f441fb4e54c58108b3fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e80f898345044f8a887429edd0f2397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611e42b34edb40d99e2be5a72b2d0f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e686047a264ceabeb8338ae5ba310f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece0ea8979ca4ccfa01f6e62d5da8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=32):   0%|          | 0/3463473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb542a3d5eed435b9c3b79f03bb48899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2417415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer'],\n",
       "    num_rows: 2417415\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_11 = load_dataset('BAAI/Infinity-Instruct',split='train', token=\"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\")\n",
    "\n",
    "# Define the filter function\n",
    "def filter_sources(batch):\n",
    "    # Check if source is not one of the specified strings\n",
    "    return [source not in {\"OpenHermes-2.5\", \"Orca-math-word-problems-200k\"} for source in batch['source']]\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_dataset_11 = dataset_11.filter(filter_sources, batched=True, num_proc=32)\n",
    "\n",
    "def extract_conversation(batch):\n",
    "    # Prepare lists for prompts and answers\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    # Process each conversation in the batch\n",
    "    for conversation in batch['conversations']:\n",
    "        prompt = \"\"\n",
    "        answer = \"\"\n",
    "        for entry in conversation:\n",
    "            if entry['from'] == 'human':\n",
    "                prompt = entry['value']\n",
    "            elif entry['from'] == 'gpt':\n",
    "                answer = entry['value']\n",
    "        # Append the results for each conversation to the lists\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    return {'prompt': prompts, 'answer': answers}\n",
    "\n",
    "# Apply the mapping function to the dataset using batching and multiple processes\n",
    "dataset_11 = filtered_dataset_11.map(extract_conversation, batched=True, num_proc=32).remove_columns([\"conversations\",\"source\", \"id\",\"target_value\"])\n",
    "dataset_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa86068-8741-4d7f-b419-2d2c2f515b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_1 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 12859\n",
      "----------------------------------------\n",
      "Dataset_2 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 109101\n",
      "----------------------------------------\n",
      "Dataset_3 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 808812\n",
      "----------------------------------------\n",
      "Dataset_4 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 200035\n",
      "----------------------------------------\n",
      "Dataset_5 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 75992\n",
      "----------------------------------------\n",
      "Dataset_6 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 200023\n",
      "----------------------------------------\n",
      "Dataset_7 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 300000\n",
      "----------------------------------------\n",
      "Dataset_8 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 211269\n",
      "----------------------------------------\n",
      "Dataset_9 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 16006\n",
      "----------------------------------------\n",
      "Dataset_10 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 27437\n",
      "----------------------------------------\n",
      "Dataset_11 details:\n",
      "Features: ['prompt', 'answer']\n",
      "Number of rows: 2417415\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset_1 to dataset_10 are loaded and accessible\n",
    "for i in range(1, 12):\n",
    "    dataset = eval(f'dataset_{i}')  # dynamically access each dataset\n",
    "    print(f'Dataset_{i} details:')\n",
    "    print(f'Features: {dataset.column_names}')\n",
    "    print(f'Number of rows: {dataset.num_rows}')\n",
    "    print('----------------------------------------')  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24516582-546b-421a-8a59-75cc86671361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset features: ['prompt', 'answer']\n",
      "Number of rows in combined dataset: 4378949\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Assume dataset_1 to dataset_10 are already loaded and have the same schema\n",
    "all_datasets = [eval(f'dataset_{i}') for i in range(1, 12)]\n",
    "combined_dataset = concatenate_datasets(all_datasets)\n",
    "\n",
    "# Printing some details about the combined dataset to confirm\n",
    "print(f'Combined dataset features: {combined_dataset.column_names}')\n",
    "print(f'Number of rows in combined dataset: {combined_dataset.num_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802b814a-2234-4b48-b244-a7ba1f61c621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c6df1057c14572812a1db29b00f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4378949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_prompt(batch):\n",
    "    # List of substrings to remove\n",
    "    substrings_to_remove = ['<|prompter|>', '<|endoftext|>', '<|assistant|>','<|endofprompt|>']\n",
    "    \n",
    "    # Process each prompt in the batch\n",
    "    cleaned_prompts = []\n",
    "    for prompt in batch['prompt']:\n",
    "        for substring in substrings_to_remove:\n",
    "            prompt = prompt.replace(substring, '')\n",
    "        cleaned_prompts.append(prompt)\n",
    "\n",
    "    # Update the batch with cleaned prompts\n",
    "    batch['prompt'] = cleaned_prompts\n",
    "    return batch\n",
    "\n",
    "# Apply the cleaning function to the dataset using batching and multiple processes\n",
    "cleaned_dataset = combined_dataset.map(clean_prompt, batched=True, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c156e8c1-c43f-4452-9658-7a7b897bb28f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbced6807f67495d8f4b406549975aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4378949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be given a definition of a task first, then some input of the task.\n",
      "This task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\n",
      "\n",
      "AFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\n",
      "Output: 150\n",
      "Dataset({\n",
      "    features: ['prompt', 'answer', 'length'],\n",
      "    num_rows: 4378949\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "# Assume `encoding` is an object that has an `encode` method (like a tokenizer)\n",
    "current_index = 0  # This will track the index of the current sample\n",
    "\n",
    "def add_token_counts(example):\n",
    "    global current_index\n",
    "    try:\n",
    "        num_tokens = len(encoding.encode(example['prompt']))\n",
    "        example['length'] = num_tokens\n",
    "    except ValueError as e:\n",
    "        print(f\"Error at index {current_index}: {str(e)}\")\n",
    "        # Optionally, you can add error handling, such as skipping this entry\n",
    "        example['length'] = 0  # Setting default or error value\n",
    "    finally:\n",
    "        current_index += 1  # Increment index whether there was an error or not\n",
    "    return example\n",
    "\n",
    "# Applying the function to the dataset\n",
    "filtered_dataset = cleaned_dataset.map(add_token_counts, batched=False)\n",
    "\n",
    "# Example of the dataset entry with token counts\n",
    "print(filtered_dataset['prompt'][0], filtered_dataset['length'][0])\n",
    "print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01873f04-e470-4c7b-8494-d8bf607c1c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea95162aca1e4e9bad49296bbc48e41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4378949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with contrain tokens: 3024117\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to keep only samples with fewer than 1024 tokens\n",
    "filtered_dataset = filtered_dataset.filter(lambda example: example['length'] < 64, batched=False)\n",
    "\n",
    "# Print the number of samples in the filtered dataset\n",
    "print(f\"Number of samples with contrain tokens: {len(filtered_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c9de2f7-02a1-4a35-afd9-4e190507928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "# Download the language identification model\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c301378-5abb-4c33-b29b-e8afebefaf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fd40c153740> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2192a6ebd5ef42d1a41acf0c356fe3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3024117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Generate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One', 'answer': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.', 'length': 56}\n",
      "Dataset({\n",
      "    features: ['prompt', 'answer', 'length'],\n",
      "    num_rows: 2356547\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define a function to check if the text is in English, handling None values\n",
    "def is_english(example):\n",
    "    text = example['prompt']\n",
    "    if text is None:\n",
    "        return False  # Skip None values\n",
    "    text = text.replace('\\n', ' ')  # Replace newlines with space\n",
    "    prediction, probability = model.predict(text)\n",
    "    return prediction[0] == '__label__eng_Latn' and probability[0] > 0.8  # Adjust threshold as needed\n",
    "\n",
    "# Apply the filter to the dataset\n",
    "english_dataset = filtered_dataset.filter(lambda example: is_english(example))\n",
    "\n",
    "# Example of the English-only dataset\n",
    "print(english_dataset[0])\n",
    "print(english_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dadebd0-d05f-46d2-94df-e53dac5ab340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# filter samples not really good as a sound samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a41aac-a832-4b53-a5b4-3a4df34180a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b857f40ae942d3a44b8788aac3b012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f394a2864894005bfb3d62141f676c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a4a189d6e44d689e612c553b04d114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d568a27793004e6e9853391b3b0f867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcda3892c12489d8f324a91846b3154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780badbb8b914ccaa2b690c043c55cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975f7b485b846c190061e68247644ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ba20378c3048e9bc9c835e7a68995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bea295ae09460f8c3645724bd9b87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/295 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b41b9349714f43b5e6065cdc4f7010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jan-hq/instruction-speech-v1.5/commit/27749e8ae7bccdbb30e31212b7721f31403e0f22', commit_message='Upload dataset', commit_description='', oid='27749e8ae7bccdbb30e31212b7721f31403e0f22', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset.push_to_hub(\"jan-hq/instruction-speech-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ee0ec2-232e-4b77-aabe-0ac72627655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"jan-hq/instruction-speech-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f3a169-77ef-4ac8-b190-bcff6c8f7d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b274ee5b33e4a98bc2a661434c993a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/2356547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'answer', 'length', 'index'],\n",
       "        num_rows: 2356547\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_index_column(dataset):\n",
    "    def add_index(example, idx):\n",
    "        example['index'] = idx\n",
    "        return example\n",
    "    return dataset.map(add_index, with_indices=True, num_proc=64)\n",
    "\n",
    "english_dataset = add_index_column(ds)\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5278cf-f714-4b54-bf65-fe0d7a7cf748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
